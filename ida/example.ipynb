{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# dswizard Example\n",
    "This notebook shows you how to use our AutoML tool dswizard. The source code is available on\n",
    "[Github](https://github.com/Ennosigaeon/dswizard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import openml\n",
    "\n",
    "from dswizard.core.master import Master\n",
    "from dswizard.core.model import Dataset\n",
    "from dswizard.optimizers.bandit_learners.pseudo import PseudoBandit\n",
    "from dswizard.optimizers.config_generators import Hyperopt\n",
    "from dswizard.optimizers.structure_generators.mcts import MCTS, TransferLearning\n",
    "from dswizard.util import util\n",
    "\n",
    "\n",
    "# Fix working directory if we are in binder\n",
    "if os.getcwd() == '/home/jovyan/ida':\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At first we have to provide which data set we want to optimize and how the optimization should be configured. In this\n",
    "example we are going to use tasks from [OpenML](https://www.openml.org/search?type=task). More specific, we will perform\n",
    "a [supervised classification](https://www.openml.org/t/146606) on the [higgs](https://www.openml.org/d/23512) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum optimization time for in seconds\n",
    "wallclock_limit = 60\n",
    "#Maximum cutoff time for a single evaluation in seconds\n",
    "cutoff = 10\n",
    "# Directory used for logging\n",
    "log_dir = 'run/'\n",
    "# OpenML task id\n",
    "task = 146606\n",
    "\n",
    "util.setup_logging(os.path.join(log_dir, str(task), 'log.txt'))\n",
    "logger = logging.getLogger()\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we will actually load the data set and extract the trainings and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logger.info('Processing task {}'.format(task))\n",
    "task = openml.tasks.get_task(task)\n",
    "train_indices, test_indices = task.get_train_test_split_indices(repeat=0, fold=0, sample=0)\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "X, y, _, _ = task.get_dataset().get_data(task.target_name)\n",
    "X_train = X.loc[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X.loc[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "ds = Dataset(X_train.to_numpy(), y_train.to_numpy(), metric='rocauc')\n",
    "ds_test = Dataset(X_test.to_numpy(), y_test.to_numpy(), metric=ds.metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the [higgs](https://www.openml.org/d/23512) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will create the actual optimizer instance. In this example we use 2 parallel workers that generate\n",
    "hyperparameters on the fly using Hyperopt. The structure is obtained using MCTS with transfer learning from previous\n",
    "evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "master = Master(\n",
    "    ds=ds,\n",
    "    working_directory=os.path.join(log_dir, str(task)),\n",
    "    n_workers=2,\n",
    "    model='../dswizard/assets/rf_complete.pkl',\n",
    "\n",
    "    wallclock_limit=wallclock_limit,\n",
    "    cutoff=cutoff\n",
    ")\n",
    "\n",
    "pipeline, run_history = master.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we will extract the best pipeline from our run history and print some evaluation statistics. More details about\n",
    "the evaluated structures and configurations is available in the `run/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, incumbent = run_history.get_incumbent()\n",
    "logging.info('Best found configuration: {}\\n{} with loss {}'.format(incumbent.steps,\n",
    "                                                                    incumbent.get_incumbent().config,\n",
    "                                                                    incumbent.get_incumbent().loss))\n",
    "logging.info('A total of {} unique structures where sampled.'.format(len(run_history.data)))\n",
    "logging.info('A total of {} runs where executed.'.format(len(run_history.get_all_runs())))\n",
    "\n",
    "logging.info('Final pipeline:\\n{}'.format(pipeline))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)\n",
    "logging.info('Final test performance {}'.format(util.score(y_test, y_prob, y_pred, ds.metric)))\n",
    "\n",
    "ensemble = master.build_ensemble()\n",
    "logging.info('Final ensemble performance {} based on {} pipelines'.format(\n",
    "    util.score(ds_test.y, ensemble.predict_proba(ds_test.X), ensemble.predict(ds_test.X), ds.metric),\n",
    "    len(ensemble.estimators_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
